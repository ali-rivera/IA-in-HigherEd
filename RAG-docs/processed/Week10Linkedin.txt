All you need to know about K-Means  
Clustering

In Supervised Learning , we are provided with a dataset having data points as well as a class label (in case of classification) or a real value (in case of regression) as output. However , in the case of Unsupervised Learning, we are not provided with the output.Clustering is one of the Unsupervised Learning method in which we group similar items together in order to form a cluster.Before diving into the algorithm , let's go through some of the applications of clustering so that so you can the essence of this algorithm.Applications1) E-Commerce:Suppose ABC company wants to give some special deals to its customers. However , different customer may require different type of deal. What the company can do is divide customers into different groups such that each group members have similar properties. And once it is done , companies can target different clusters and offer different schemes as per requirement.2) Image segmentation :It is nothing but grouping of pixels of an image.It is used to seperate regions in an image.3) Automate manual filling of dataSuppose you are preparing a dataset for Twitter Sentiment Analysis ( or any other dataset which requires human intelligence). In this dataset , you would read each of the tweets and assign polarity (negative or positive). Imagine you want to prepare a dataset with 100K datapoints. Manually assigning the polarity would require a lot of time and cost. In that case, we'll form let's say 1K clusters and manually assign only for 1 point (key point) taken from each of these 1K clusters. Rest of the points in a cluster will be assigned the same polarity as that of the key point of that cluster.Performance metric for Clustering algorithmsThe way we judge performance of any clustering algorithm is using two quantities:Inter-cluster distance : we want distances between clusters to be as large as possible.Intra-cluster distance : we want distances between points within a cluster to be as small as possible.One of such metrics is known as Dunn Index.It is defined asLet's break it downd(i,j) := Inter-cluster distance between two clusters i and jand d' (k) := Intra-cluster distance of cluster kmeans minimum Inter-cluster distance amongst all Inter-cluster distancesandmeans maximum Intra-cluster distance amongst all Intra-cluster distances.(I'll show you how to calculate these distances in the later part of this article)If you look at the numerator , we want it to be as large as possible and for denominator, it should be as small as possible.Hence, High Dunn Index means better clustering algorithm.WAIT .... Didn't I just say that we want maximum Inter-cluster distance and minimum Intra-cluster distance? Then why are we taking the opposite in Dunn Index calculation ?Well , that's how Dunn Index works. It takes into consideration the worst clusters that are present and if Dunn Index comes high even for worst clusters, then we can assume our clustering algorithm has done a good work.Calculating min(Inter-cluster distances) and max(Intra-cluster distances)For min(Inter-cluster distances) , we calculate min(Inter-cluster distance) between every pair of clusters and out of those distances , and then we'll select the minimum one.For max(Intra-cluster distances) , we calculate max(Intra-cluster distance) inside every cluster and out of those distances , and then we'll select the maximum one.Intuition behind K-Means clustering algorithmHere, k (hyperparameter) is the number of clusters that we want . This has to be provided to our algorithm. We'll see how to find the right value of k in the later part of this article.We'll first divide datapoints into k nearly equal groups (Cluster) and for each of these groups , we'll find a centroid which is nothing but geometric center of points in a groupLet C‚ÇÅ,C‚ÇÇ,C‚ÇÉ ,....., C‚Çñ be k Centroids and S‚ÇÅ,S‚ÇÇ,S‚ÇÉ ,....., S‚Çñ be k sets (clusters) of points corresponding to the centroids.We'll now pick each point one by one, find its distance from all of these k centroids , assign the point to that cluster for which the distance from point to the corresponding centroid is minimumAn obvious question:What if number of dimensions is very high. Euclidean distance fails in that case. How do we deal with that?1) We can use other versions of K-means like Spherical K-Means which uses Cosine Similarity instead of Euclidean distance.2) We can first reduce the dimensionality and then proceed with our regular K-means or Spherical K-means.Mathematical formulation of K-Means ClusteringLet's decode this -First summation is for the k clusters .Next summation is for each of the points in a particular cluster.First constraint says that each point must belong to a clusterSecond constraint says that a point can belong to only one clusterSo if you look carefully , we'll get k centroids as our output which minimizes Intra-cluster distance.Easy?Sadly, this optimization is not easy to solve. In fact , this is an NP-hard problem.Hence, we use an approximation algorithm in order to solve this. In this article , we are going to discuss one such approximation algorithm known as Lloyd Algorithm.Lloyd's algorithm:Step 1) Random Initialization:Take random k points from the dataset as centroidsStep 2) Assignment:Step 3) Update centroids:Recalculate C‚±º 's as follows:which is nothing but calculating mean of all the points in the cluster.Step 4) Repeat steps 2 and 3 until centroids don't change much i.e. distance between old centroids and new ones is very small.The problem with Simple K-Means ApproachWe've seen how we can solve our optimization problem using Lloyd's algorithm , however, there lies a problem with the 1st step. Can you guess?It involves random initialization , which means the results which we'll get are initialization sensitive i.e. results will change as we change the initialization. This may lead to an undesirable result.How to deal with it?Well, there are two ways.1) Run K-Means multiple times using random initialization and finally pick the one which gives highest Dunn Index (or any other performance metric )2) Apply K-Means ++K-Means ++You can think of it like K-Means with a smarter approach to pick initial centroids C‚ÇÅ,C‚ÇÇ,C‚ÇÉ ,....., C‚ÇñLet's understand what is it about ,step by step.Step 1) Pick a random point from the dataset as C‚ÇÅ.Step 2) Now take each of the point from D - {C‚ÇÅ} and calculate square of the distance from the nearest centroid.Step 3) Select a point from D - {C‚ÇÅ} in such a way that the probability of selecting a point is proportional to the square of the distance which was calculated in previous step. That point will be centroid C‚ÇÇ.Step 4) Repeat the steps 2 and 3 till we find all of our k initial centroids.Rest of the steps are same as Simple K-Means.How K-Means ++ is going to help us ?Chances of selecting centroids which are far away from each other will increase.It will definitely be affected by outliers however the chances are reduced because of the probabilistic approach applied in step (3) of the above algorithm.Limitations of K-MeansWe have to explicitly specify the number of clusters we want.It doesn't work well with clusters of different sizes and densities.It doesn't work well with non-convex / non-globular shapes.Outliers can affect the result.Determining the right K-value1) Domain Knowledge :If you take the example of Twitter Sentiment Analysis , we know there are 2 cases possible , either positive or negative . Hence , we select K = 22) Elbow MethodWe know the loss function of K-Means is :If we draw a curve taking different values of K and the corresponding loss , we'll get something like this:-and the elbow point will give us the optimal K value.Time and Space Complexity for K-MeansLet, i = number of iterations ‚†Ä‚†Ä d = dimension of points‚†Ä‚†Ä‚†Ä ‚†Ä‚†Ä‚†Ä‚†Ä ‚†Ä‚†Ä‚†Ä‚†Ä ‚†Ä‚†Ä‚†Ä‚†Ä ‚†Ä‚†Ä‚†Ä‚†Ä ‚†Ä‚†Ä‚†Ä‚†Ä ‚†Ä‚†Ä‚†Ä‚†Ä ‚†Ä ‚†Ä‚†Ä k = number of clusters = number of centroids ‚†Ä‚†Ä‚†Ä ‚†Ä‚†Ä‚†Ä‚†Ä ‚†Ä‚†Ä‚†Ä‚†Ä ‚†Ä‚†Ä‚†Ä‚†Ä ‚†Ä‚†Ä‚†Ä‚†Ä ‚†Ä‚†Ä‚†Än = number of datapointsTime complexity = O( i * n * k * d)Because for each iteration , we calculate distance of each point from the centroids and calculating distance required operations on d dimensionsWhen k is very small and number of iterations are also pre-defined. In that case , Time complexity ‚âà O( nd )Space complexity = O( nd + kd)Because we have to store all of the n points as well as k centroidsTypically , k is very small as compared to number of points . Hence , Space complexity ‚âà O( nd )NOTE: This K-Means algorithm will not work well with categorical variables. When dealing with categorical variables one may use different distance measures like Hamming distance or one can switch to different versions of K-Means like K-Modes Clustering.Congrats . You have successfully added K-Means to your Data Science toolkit üòÄThat's all for now !!!!
