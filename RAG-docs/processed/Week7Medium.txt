Evaluation Metrics — Supervised ML | by Manpreet Buttar | MediumOpen in appSign upSign inWriteSign upSign inEvaluation Metrics — Supervised MLManpreet Buttar·Follow8 min read·Feb 15, 2024--ListenShareFor supervised learning models, evaluation typically involves comparing the predictions made by the model with the ground truth labels that are provided in the dataset. Here are some common evaluation metrics used for assessing the performance of supervised learning models:Classification Metrics — Classification models predict dependent discrete variables.Confusion Matrix                          Predicted Class                | Positive             | Negative              | — — — — — — — — — — — — — — — — — — — — — - - - - -- - - - -- - - - -Actual Class        Positive | True Positive (TP)   | False Negative (FN)   |       Negative | False Positive (FP)  | True Negative (TN)    | > True Positive (TP): the model correctly predicts the positive class.> True Negative (TN): the model correctly predicts the negative class.> False Positive (FP): Also known as a Type I error. The model incorrectly predicts the positive class when the  actual class is negative.> False Negative (FN): Also known as a Type II error.  The model incorrectly predicts the negative class when the actual class is positive.Accuracy: [(TP + TN) / (TP + TN + FP + FN)]Measures the proportion of correctly classified instances out of the total instances. It’s suitable for balanced datasets but can be misleading for imbalanced datasets. Higher accuracy values indicate a higher proportion of correct predictions.Precision: TP / (TP + FP) or predicted positivesMeasures the proportion of true positive predictions out of all positive predictions. Higher precision values indicate fewer false positive predictions. Precision is useful when the cost of false positives is high.Recall (Sensitivity)/TPR: TP / (TP + FN) or actual positivesMeasures the proportion of true positive predictions out of all actual positives. It focuses on the ability of the model to capture positive instances. Higher recall values indicate fewer false negative predictions.Specificity/TNR : TN / (TN + FP) or actual negativesSpecificity, also known as the true negative rate, is a metric used in binary classification tasks to evaluate the performance of a model in correctly identifying negative instances (actual negatives). Higher specificity values indicate fewer false positive predictions.False Positive Rate (FPR): FP / (FP + TN) or actual negativesAlso known as the false alarm rate, it is a metric used in binary classification tasks to evaluate the performance of a model in correctly identifying negative instances. It is the complement of specificity. Lower FPR values indicate fewer false positive predictions.F1-score: 2 * (Precision * Recall) / (Precision + Recall)The harmonic mean of precision and recall provides a balance between the two metrics. It’s useful when there’s an uneven class distribution.from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix# Example ground truth and predicted labelsy_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]y_pred = [1, 1, 1, 0, 1, 1, 0, 0, 0, 1]# Calculate accuracyaccuracy = accuracy_score(y_true, y_pred)print("Accuracy:", accuracy)# Calculate precisionprecision = precision_score(y_true, y_pred)print("Precision:", precision)# Calculate recallrecall = recall_score(y_true, y_pred)print("Recall:", recall)# Calculate F1-scoref1 = f1_score(y_true, y_pred)print("F1-score:", f1)# Calculate confusion matrixconf_matrix = confusion_matrix(y_true, y_pred)print("Confusion Matrix:")print(conf_matrix)# Calculate False Positive Rate (FPR)TN = conf_matrix[0, 0]FP = conf_matrix[0, 1]FPR = FP / (FP + TN)print("False Positive Rate (FPR):", FPR)ROC Curve and AUC: The Receiver Operating Characteristic (ROC) curve plots the true positive rate (TPR, also known as Sensitivity) against the false positive rate (FPR) at various threshold settings. Area Under the ROC Curve (AUC) measures the entire two-dimensional area under the ROC curve. It ranges from 0 to 1, where a score closer to 1 indicates better discrimination ability of the model.> Threshold: Threshold refers to a value that is used to convert predicted probabilities into binary predictions (0 or 1). Instances with predicted probabilities above the threshold are classified as positive (1), while those below the threshold are classified as negative (0).> Log loss — It is a negative average of the log of corrected predicted probabilities for each instance. It calculates the negative log-likelihood of the predicted probabilities compared to the true labels. Lower Log Loss values indicate better predictive performance, with 0 being the optimal score (perfect predictions).from sklearn.metrics import roc_curve, roc_auc_score, log_lossimport matplotlib.pyplot as plt# Assuming y_true and y_prob are the true labels and predicted probabilities, respectively# Replace y_true and y_prob with your actual datay_true = [0, 1, 0, 1]  # Example true labelsy_prob = [0.1, 0.9, 0.3, 0.7]  # Example predicted probabilities# Compute ROC curvefpr, tpr, thresholds = roc_curve(y_true, y_prob)# Compute ROC AUC scoreroc_auc = roc_auc_score(y_true, y_prob)# Plot ROC curveplt.figure()plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')plt.xlim([0.0, 1.0])plt.ylim([0.0, 1.05])plt.xlabel('False Positive Rate')plt.ylabel('True Positive Rate')plt.title('Receiver Operating Characteristic (ROC) Curve')plt.legend(loc='lower right')plt.show()# Compute Log Losslogloss = log_loss(y_true, y_prob)print(f'Log Loss: {logloss:.4f}')Regression Metrics — Regression models predict dependent continuous variables.Mean Absolute Error (MAE): MAE measures the average absolute difference between the predicted values and the true values. It is less sensitive to outliers compared to other metrics like MSE.Mean Squared Error (MSE): MSE measures the average squared difference between the predicted values and the true values. It amplifies large errors more than smaller ones.Root Mean Squared Error (RMSE): RMSE is the square root of the MSE. It is in the same units as the target variable, making it interpretable.Mean Absolute Percentage Error (MAPE): MAPE measures the average percentage difference between the predicted values and the true values. It is expressed as a percentage.R-squared (R2) Score: The coefficient of determination, R2, represents the proportion of the variance in the dependent variable that is explained by the independent variables in the model. It ranges from 0 to 1, where 1 indicates a perfect fit and 0 indicates no improvement over a constant baseline model.Adjusted R2: Adjusted R2 adjusts R2 for the number of predictors (independent variables) in the model. It penalizes the addition of unnecessary predictors and is often preferred over R2 when comparing models with different numbers of predictors.import numpy as np# Define the true values and predicted valuesy_true = np.array([10, 20, 30, 40, 50])y_pred = np.array([12, 18, 32, 45, 55])# Mean Absolute Error (MAE)mae = np.mean(np.abs(y_true - y_pred))# Mean Squared Error (MSE)mse = np.mean((y_true - y_pred) ** 2)# Root Mean Squared Error (RMSE)rmse = np.sqrt(mse)# Mean Absolute Percentage Error (MAPE)mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100# R-squared (R2)ss_res = np.sum((y_true - y_pred) ** 2)ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)r2 = 1 - (ss_res / ss_tot)# Adjusted R-squared (adjusted R2)n = len(y_true)k = 1  # number of predictors (independent variables), assuming one predictor for simplicityadjusted_r2 = 1 - ((1 - r2) * (n - 1) / (n - k - 1))# Display the resultsprint("Mean Absolute Error (MAE):", mae)print("Mean Squared Error (MSE):", mse)print("Root Mean Squared Error (RMSE):", rmse)print("R-squared (R2):", r2)print("Adjusted R-squared (adjusted R2):", adjusted_r2)Cross-Validation: Techniques like k-fold cross-validation and stratified cross-validation are used to assess the model’s generalization performance by splitting the dataset into multiple subsets for training and testing.Cross-validation is a technique that involves partitioning the dataset into multiple subsets (folds), training the model on some of the folds, and evaluating it on the remaining fold(s).The most common type of cross-validation is k-fold cross-validation, where the dataset is divided into k equal-sized folds. The model is trained k times, each time using k-1 folds for training and the remaining fold for validation.Cross-validation helps to provide a more accurate estimate of the model’s performance compared to a single train-test split, as it uses multiple train-test splits and averages the results.It is useful for assessing how well a model generalizes to new, unseen data.from sklearn.model_selection import cross_val_scorefrom sklearn.datasets import load_irisfrom sklearn.linear_model import LogisticRegression# Load the Iris datasetiris = load_iris()X, y = iris.data, iris.target# Initialize the modelmodel = LogisticRegression()# Perform cross-validationscores = cross_val_score(model, X, y, cv=5)  # 5-fold cross-validation# Print the cross-validation scoresprint("Cross-validation scores:", scores)# Calculate and print the mean accuracyprint("Mean accuracy:", scores.mean())Bias-Variance Tradeoff: Evaluating the bias and variance of the model helps in understanding whether the model is underfitting, overfitting, or performing optimally. It manages the tradeoff between model simplicity and generalization performanceBias measures the error introduced by approximating a real-world problem with a simplified model. High-bias models may underfit the training data and fail to capture the underlying patterns.Variance measures the model’s sensitivity to fluctuations in the training data. High variance models may overfit the training data and perform poorly on new, unseen data.The tradeoff arises because reducing bias often increases variance and vice versa. Finding the right balance between bias and variance is crucial for building models that generalize well to new data.Techniques like regularization, cross-validation, and model selection can help manage the bias-variance tradeoff by controlling the complexity of the model and assessing its performance on different datasets.Hyperparameters — Hyperparameters are parameters that are not directly learned from the data during the training process but are set prior to training.Grid Search: In Grid Search, we specify a grid of hyperparameter values to try out. For each combination of hyperparameters in the grid, we train the model and evaluate its performance using cross-validation. Grid Search exhaustively searches through all combinations of hyperparameters and selects the combination that yields the best performance according to a specified evaluation metric.Random Search: In Random Search, instead of searching through all possible combinations of hyperparameters, we randomly select a certain number of combinations to evaluate. Random Search randomly samples hyperparameter values from specified distributions. This approach is less computationally intensive compared to Grid Search, especially when dealing with a large search space of hyperparameters.from sklearn.model_selection import GridSearchCV, RandomizedSearchCVfrom sklearn.ensemble import RandomForestClassifier # RandomForestRegressor used for cont variablefrom sklearn.datasets import load_iris# Load datasetiris = load_iris()X, y = iris.data, iris.target# Define modelmodel = RandomForestClassifier() # define your model here# Grid Searchparam_grid = {'n_estimators': [50, 100, 200],              'max_depth': [None, 5, 10]}grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)grid_search.fit(X, y)print("Best parameters (Grid Search):", grid_search.best_params_)# redo the model with the best parameters# Random Searchrandom_param_dist = {'n_estimators': [50, 100, 200],                     'max_depth': [None, 5, 10]}random_search = RandomizedSearchCV(estimator=model, param_distributions=random_param_dist, n_iter=3, cv=5)random_search.fit(X, y)print("Best parameters (Random Search):", random_search.best_params_)# redo the model with the best parametersEstimator (n_estimators): The n_estimators parameter specifies the number of decision trees (estimators) to include in the random forest.Depth (max_depth): The max_depth parameter controls the maximum depth/length of each decision tree in the random forest. A deeper tree can capture more intricate relationships in the training data, potentially leading to overfitting. Conversely, a shallower tree may result in underfitting.Feature (max_features): The max_features parameter determines the maximum number of features to consider when looking for the best split at each node of the decision tree. Choosing a smaller number of features can help reduce overfitting by introducing randomness into the model. It’s often set to the square root of the total number of features, other values could be log2, auto, or none.Domain-Specific Metrics:In some domains, domain-specific metrics might be more relevant. For example, in healthcare, metrics like sensitivity, specificity, and AUC-ROC are commonly used.It’s important to select evaluation metrics that are appropriate for the specific task and dataset characteristics. Additionally, considering multiple evaluation metrics provides a more comprehensive understanding of the model’s performance.Machine LearningEvaluation MetricQa TestingData ScienceData Visualization----FollowWritten by Manpreet Buttar5 FollowersFollowHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams
























